# 深度学习笔记  

## 词袋模型

将语言想象成一个词袋，其中每个词都是一个特征。例如

- That is a cute dog.  
- My cat is cute.  

Split input by a whitespace;(this process is called tokenization)  
By tokenizing all the input text, we can obtain a token vocabulary.  
根据词汇表`token vocabulary`，我们可以将每个句子转换为一个数值向量，其中每个数值元素表示该单词在句子中出现的次数，以此创建了词袋模型`a bag of words model`.

## Word嵌入(Vector Embedding)  

> `Word2Vec`  
>
> - 一种用于学习单词嵌入的无监督学习算法。通过学习单词在句子中出现的位置生成词嵌入`word emdeddings`。
> - 它的目标是将单词映射到一个高维的连续向量空间中，使得语义相似的单词在向量空间中距离较近。  
> - `Word2Vec`模型可以分为两种：`CBOW（Continuous Bag-of-Words）`和`Skip-gram`。
> - `CBOW`模型的目标是根据上下文单词来预测目标单词，而Skip-gram模型的目标是根据目标单词来预测上下文单词。  
> - 忽略了上下文单词之间的顺序，只考虑了单词之间的关系，没考虑文本的语义。  

word embedding 是一种表示模型`representation model`，将单词表示为数值（向量）的方法，这些向量可以捕捉单词之间的语义关系。该向量有具体固定的维度，不同的维度对应不同的语义特征。具有相似意义的单词词嵌入向量之间的距离也比较接近。相似程度取决于训练数据。

## 编码和解码上下文  

这种架构的顺序性质排除了在模型训练期间的并行化。  

## Transformer模型  

- 完全基于注意力机制，不需要循环神经网络或卷积神经网络。  
- 允许模型并行训练，可以并行处理输入序列，这使得模型训练和推理速度更快。
- 可以处理长序列，处理不同长度的序列，处理序列中的不同类型的元素，而不需要使用截断或填充。  

### 工作方式  

Transformer模型由多个编码器`transformer encoder blocks`和解码器`transformer decoder blocks`堆叠组成。  

- 编码器`transformer encoder blocks`：

将输入文本转换成嵌入向量`embeddings`，不是用wordtovec而是从随机值开始，然后将这些嵌入向量输入到多个编码器`transformer encoder blocks`中，每个编码器`transformer encoder blocks`由多个自注意力机制`self-attention mechanisms`和前馈神经网络`feed-forward neural networks`组成。

- 解码器`transformer decoder blocks`：

将编码器`transformer encoder blocks`的输出作为解码器的输入，解码器`transformer decoder blocks`也由多个自注意力机制`self-attention mechanisms`和前馈神经网络`feed-forward neural networks`组成。

- 自注意力机制`self-attention mechanisms`：

自注意力机制`self-attention mechanisms`是一种计算输入序列中每个元素之间的相关性的方法。它通过计算输入序列中每个元素与其他元素之间的相关性，然后将这些相关性作为权重，应用于输入序列中的每个元素，从而生成一个新的序列。

## Tokenizers分词器

## 架构概述  

## the Transformer Block变压器块  

## 自我关注  

## 模型示例

## 最近的改进

## 混合专家

## Conclusion  
