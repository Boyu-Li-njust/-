# 深度学习笔记  

## 词袋模型

将语言想象成一个词袋，其中每个词都是一个特征。例如

- That is a cute dog.  
- My cat is cute.  
Split input by a whitespace;. (this process is called tokenization)  
By tokenizing all the input text, we can obtain a token vocabulary.  
根据词汇表`token vocabulary`，我们可以将每个句子转换为一个数值向量，其中每个数值元素表示该单词在句子中出现的次数，以此创建了词袋模型`a bag of words model`.

## Word嵌入(Vector Embedding)  

> `Word2Vec`  
>
> - 一种用于学习单词嵌入的无监督学习算法。通过学习单词在句子中出现的位置生成词嵌入`word emdeddings`。
> - 它的目标是将单词映射到一个高维的连续向量空间中，使得语义相似的单词在向量空间中距离较近。  
> - `Word2Vec`模型可以分为两种：`CBOW（Continuous Bag-of-Words）`和`Skip-gram`。
> - `CBOW`模型的目标是根据上下文单词来预测目标单词，而Skip-gram模型的目标是根据目标单词来预测上下文单词。  
> - 忽略了上下文单词之间的顺序，只考虑了单词之间的关系，没考虑文本的语义。  

word embedding 是一种表示模型`representation model`，将单词表示为数值（向量）的方法，这些向量可以捕捉单词之间的语义关系。该向量有具体固定的维度，不同的维度对应不同的语义特征。具有相似意义的单词词嵌入向量之间的距离也比较接近。相似程度取决于训练数据。

## 编码和解码上下文  

## Transformer模型  

## Tokenizers分词器

## 架构概述  

## the Transformer Block变压器块  

## 自我关注  

## 模型示例

## 最近的改进

## 混合专家

## Conclusion  
